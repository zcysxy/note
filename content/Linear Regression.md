---
{"publish":true,"aliases":"Overdetermined System","created":"2022-02-22T20:20:09","modified":"2025-08-05T21:56:05","tags":["pub-stat"],"cssclasses":"","type":"note","sup":["[[Regression]]","[[Machine Learning]]"],"state":"done","related":["[[Ridge Regression]]"]}
---


# Linear Regression

Linear regression adopts a linear-in-parameter regression function model $\{ f_{w} \}_{w\in\R^{n}}$ such that
$$\hat{y} = f_{w}(x) = w^Tx.$$
In this note, we focus on real-valued output $y\in\R$. We can think of $w$ as a set of **weights** that determine how each feature affects the prediction.

Linear regression is the *correct* model for data generated by a ==linear model==:
$$
Y_{i} = w^TX_{i} + \epsilon _{i},
$$
where $\epsilon _{i}$ is a zero-mean random noise. If the noise follows a [[Normal Distribution\|Gauss distribution]], we get a [[Gaussian Linear Model]], whose estimator of $w$ have nice properties.

Suppose we have collected $n$ sample points $(X_{i},Y_{i})$, the matrix form of the model is
$$
Y = Xw + \epsilon,
$$
where $X = (X_{1},\dots,X_{n})^{T}\in\R^{n\times d}$ is called the ==design matrix==, $Y=(Y_{1},\dots,Y_{n})^T\in\R^{n}$ is the output vector, and $\epsilon=(\epsilon_{1},\dots,\epsilon _{n})^T\in\R^{n}$ is the noise vector. Regression focuses on fitting the collected (training) data, corresponding to a ==fixed design== problem; [[Prediction]] focuses on predicting the new (test) data, corresponding to a ==random design== problem.

## Ordinary Least Squares

We use [[Mean Squared Error]] as the performance measure
$$\operatorname{MSE}_{\mathrm{train /test}} = \frac{1}{m}\sum_i(\hat{y}_i^{(\mathrm{train /test})} - y_i^{(\mathrm{train /test})})^2 = \frac{1}{m}\|{\bf\hat{y}^{(\mathrm{train /test})} - \bf{y}^{(\mathrm{train /test})}}\|_2^2$$ ^obj

==Ordinary least squares== linear regression with a fixed design minimizes the $\operatorname{MSE_{train}}$. From now on we drop the $(\mathrm{train})$ superscript.
The solution is just the ==least squares estimator (LSE)== of $w$ ([[M-Estimator]] with [[Risk]] being MSE). Since the MSE is convex, we have
$$\begin{aligned}
&\nabla_w \operatorname{MSE}_{\mathrm{train}} = 0\\
\Rightarrow \ \ & \nabla_w \| {{X}w - y}\|_2^2 = 0\\
\Rightarrow \ \ & \nabla_w  \left({{X}w - y}\right)^T\left({{X}w - y}\right) = 0\\
\Rightarrow \ \ & 2{ X}^T{ Xw} - 2 { X}^T{ y}= 0\\
\Rightarrow \ \ & { w} = \left({ X}^T{ X}\right)^{-1}{ X}^T{ y}
\end{aligned}$$
^sol

The last set of equations is known as ==normal equations==.
Clearly, the least squares estimator requires $X$ to be full column rank, meaning that there are more samples than features.
When there are more features than samples, which is common in the high-dimensional setting, we have an [[Underdetermined Linear System]] and there are infinitely many solutions with the lest square error, and we usually seek the ==least norm== solution.

- [~] The geometric perception of the solution is $Xw_{\mathrm{LS}} = \operatorname{proj}_{\operatorname{span}X}y$. Therefore, $w_{\mathrm{LS}}$ achieves the optimality. ^69df70

### Misspecified Model

Consider a general non-parametric statistical model, $(X_i, Y_i) \overset{\text{iid}}{ \sim } P \in \mathcal{P} \{ P : P \in \operatorname{dist}(\mathbb{R}^{d+1}) \text{ w/ finite fourth moment} \}$ .
The least square solution is targeting $w^* = \operatorname*{argmin}_w \mathbb{E}[(Y_i - X_i^\top w)^2]$, i.e., *best* linear model of $Y_i$ given $Z_i$. By [[Law of Large Numbers\|LLN]] and [[Convergence of Random Variables#Continuous Mapping Theorem\|CMT]], $\hat{w}^{\mathrm{LS}} \xrightarrow{P} w^*$. Further $\hat{w}^{\mathrm{LS}}$ is asymptotically normal with some asymptotic variance $\Sigma$, which consists of the sample noise and misspecification noise.

More specifically, let's look at the expression of $w^{*}$. Similar to the derivation of the normal equation, we have
$$
\begin{aligned}
&\nabla _{w} \mathbb{E}[(Y-X^Tw)^{2}] = 0\\
\implies & \nabla _{w}\left( \mathbb{E}Y^{2} - 2 w^T \mathbb{E}XY + w^T(\mathbb{E}XX^T)w \right)  = 0 \\
\implies & w^{*} = \left(\mathbb{E}XX^T\right)^{-1}\mathbb{E}XY.
\end{aligned}
$$
To incorporate the intercept, we can add a constant $1$ to the first dimension of each observation $X' = (1; X)$. Then $w^{*} = (a^{*},b^{*})$, where $a^{*}\in\R$ is the intercept and $b^{*}\in\R^{d}$ is the slope. And using the [[Schur Complement]], we have
$$
\begin{pmatrix}
a^{*}\\b^{*}
\end{pmatrix} = \begin{pmatrix}
1 + \mathbb{E}X^T \Cov(X)^{-1}\mathbb{E}X & -\mathbb{E}X ^T \Cov(X)^{-1}\\ -\Cov(X)^{-1}\mathbb{E} X & \Cov(X)^{-1}
\end{pmatrix} \begin{pmatrix}
\mathbb{E}Y \\ \mathbb{E}XY
\end{pmatrix} = \begin{pmatrix}
\mathbb{E}Y + \mathbb{E}X^T \Cov(X)^{-1}\Cov(X,Y) \\ \Cov(X)^{-1}\Cov(X,Y)
\end{pmatrix}.
$$
We can see that the slope measures the correlation between $Z$ and $Y$.

Let's define the residual $\epsilon _{i} = Y_{i} - X_i^Tw^{*}$. Then, with the additional constant component, we have $\mathbb{E}\epsilon _{i} = 0$ and
$$
\Cov(X_{i},\epsilon _{i}) = \Cov(X_{i},Y_{i}) - \Cov(X_{i})w^{*} = \mathbf{0}.
$$
Therefore, we can see that although the noise $\epsilon$ now consists of both the sample noise and the misspecification noise, it is still uncorrelated with the input $X$ and has zero mean.

- [!] Importantly, zero correlation does not imply independence. Clearly, for a misspecified model, $Y - X^Tw^{*}| X=x$ can have a larger variance and bias for some $x$ than others, and the noise $\epsilon$ is not independent of $X$. The intercept balances out the overall bias and the slope turns *perpendicular* to the residual, which is consistent with the geometric perception of LSE.

### Bias

We can see that it is more natural and general to use **affine functions** instead of linear functions as regression functions:
$$\hat{y} = w^Tx + b$$
where $b\in\R$, is called the ==bias parameter== or the ==intercept term==, for the prediction is biased toward being $b$ in the absence of any input. This term is important to achieve an zero-mean residual ([[Linear Regression#Misspecified Model]]).

Attaching a constant 1 to the first dimension of each observation $x_i$ is a simple way to adopt this bias term: $\hat{y} = (b; w)^{T}(1;x)$. Thus, linear regression encompasses affine functions.

## A Probabilistic View: Maximum Likelihood Estimation

Suppose our residual noise follows a [[Normal Distribution\|Gaussian]] distribution $\epsilon \sim \mathcal{N}(0,\sigma^{2}I)$, then our statistical model is $y \given x \sim \mathcal{N}(w^Tx, \sigma^{2})$ parameterized by $w$. Or, given a fixed design, $Y\sim \mathcal{N}(Xw,\sigma^{2}I)$. ^c69bce

Then, the [[Maximum Likelihood Estimation]] of $w$ is
$$\begin{aligned}
    w &= \argmax_w \sum_i \log P(Y_i | X_i; w) \\
    &=\argmax_w \left(-m\log \sigma - \frac{m}{2}\log(2\pi) - \sum_i \frac{\|\hat{Y}_i - Y_i\|^2}{2\sigma^2}\right)\\
    &=\argmin_w \sum_i \|\hat{Y}_i - Y_i\|^2\\
    &=\argmin_w \|Y - Xw\|^2\\
    &=\argmin_w \operatorname{MSE}
\end{aligned}$$
^mle-obj

Therefore, if the data-generating model is a [[Gaussian Linear Model]], i.e., the noise is an independent/multivariate [[Normal Distribution\|Gaussian]] noise, LSE is equivalent to MLE.
Further, under this assumption, $w_{\mathrm{MLE}}$ is an unbiased estimate of $w$:
$$
\mathbb{E}[w_{\mathrm{MLE}}] = \mathbb{E}[(X^{T}X)^{-1}X^{T}y] = (X^{T}X)^{-1}X\mathbb{E}[y] = (X^{T}X)^{-1}X^TXw = w.
$$

^biass

The variance of MLE is
$$
\begin{aligned}
\Var[w_{\mathrm{MLE}}]
=& \mathbb{E}[w_{\mathrm{MLE}}w_{\mathrm{MLE}}^{T}] - \mathbb{E}[w_{\mathrm{MLE}}]\mathbb{E}[w_{\mathrm{MLE}}]^{T}\\
= & \mathbb{E}\left[\left(X^T X\right)^{-1} X^T Y Y^T X\left(X^T X\right)^{-1}\right]-w w^T \\
= & \left(X^T X\right)^{-1} X^T \mathbb{E}\left[Y Y^T\right] X\left(X^T X\right)^{-1}-w w^T \\
= & \left(X^T X\right)^{-1} X^T (\Var(Y) + \mathbb{E}[Y]\mathbb{E}[Y]^{T}) X\left(X^T X\right)^{-1}-w w^T \\
= & \left(X^T X\right)^{-1} X^T\left(\sigma^2 I+X w w^T X^T\right) X\left(X^T X\right)^{-1}-w w^T \\
= & \left(X^T X\right)^{-1} X^T \sigma^2 I X\left(X^T X\right)^{-1}+ \\
& \left(X^T X\right)^{-1} X^T X w w^T X^T X\left(X^T X\right)^{-1}-w w^T \\
= & \sigma^2\left(X^T X\right)^{-1}\\
= & \sigma^{2}V\Sigma ^{-2}V^{T},
\end{aligned}
$$

^var

where $X = U\Sigma V^{T}$ is the [[Singular Value Decomposition]] of $X$.
And thus the final MSE is
$$
\operatorname{MSE}(w_{\mathrm{MLE}}) = \mathbb{E}\|w_{\mathrm{MLE}}-w\|^{2}
= \mathbb{E} \tr((w_{\mathrm{MLE}} - w)(w_{\mathrm{MLE}} - w)^{T}))
= \tr(\Var (w_{\mathrm{MLE}}))
= \sigma^{2}\tr(\left(X^T X\right)^{-1}).
$$

- [!] When the data are highly correlated, there will be small singular values, the values of $w_{\mathrm{MLE}}$ are very sensitive to the measured data $y$ and give unstable predictions for new data. This is bad if we want to analyze and predict using $w_{\mathrm{MLE}}$.

[[Ridge Regression]] can help with this problem.

> [!rmk] Why "squares"?
> 
> The nice properties of least squares estimation and its connection to [[Normal Distribution\|Gaussian distribution]] are not coincidences. The hidden player is the **Euclidean geometry**. Recall that an alternative definition of the [[Normal Distribution#Equivalent Definitions for Multivariate Normal Distribution\|Gaussian distribution]] is that it's some affine transformation of some **rotation**-invariant distribution; and the [[Linear Regression#^69df70\|geometric perception]] of LSE is that it is doing orthogonal **projection**. Here, both rotation and projection are with respect to the Euclidean geometry. More specifically, both Gaussian distributions and the $\ell_{2}$ norm in [[Mean Squared Error\|MSE]] are defined using the natural [[Inner Product]] of a self-dual vector space, which is the Euclidean space, giving natural quadratic (bilinear) forms.

## Polynomial Regression

We can also use a polynomial function of degree *n* to fit the 2-D data. Then our estimation becomes

$$\hat{y} = w_nx^n + \dots + w_1x + b$$ ^poly

- [!] In polynomial regression, $n$ is often a [[Hyperparameter]]

The polynomial regression is the generalization of linear regression with bias parameter; however, it can be turned into a linear regression:
$$\hat{y} = \tilde{w}^T\tilde{x} \triangleq (b,w_1,\dots,w_n) (x^0,x^1,\dots,x^n)^T$$

- [~] Thus we can solve the polynomial regression in the same way as solving the linear regression. Actually, by the definition of linear regression, polynomial regression is still linear.

### High-Dimensional Data

Polynomial regression can also apply to high-dimensional data. Similarly, we can extend an observation $x = (x_{1}, \dots, x_{d})$ to
$$
\tilde{x} = (x_{1},\dots,x_{d}, x_{1}^{2},\dots,x_{d}^{2},\dots).
$$

We can also add features like $x_{1}x_{2}$.

## Generalized Linear Regression

By the definition of linear regression, the general form of linear regression is
$$
y_i \approx f\left(x_i, w\right)=\sum_{s=1}^S g_s\left(x_i\right) w_s = w^{T}G(x_i).
$$
For example,
$$
\begin{aligned}
& g_s\left(x_i\right)=x_{i j}^2 \\
& g_s\left(x_i\right)=\log x_{i j} \\
& g_s\left(x_i\right)=\mathbb{I}\left(x_{i j}<a\right) \\
& g_s\left(x_i\right)=\mathbb{I}\left(x_{i j}<x_{i j^{\prime}}\right)
\end{aligned}
$$

As long as the function is linear in $w$, we can construct the matrix $X$ by putting the transformed $x_i$ on row $i$, and solve $w_{\mathrm{LS}} =(X^TX)^{−1}X^Ty$.

- [!] As the number of functions increases, we need more data to avoid [[Overfitting and Underfitting\|overfitting]].

The indicator functions are called ==dummy variables==, and the functions that depend on more than one factor (like the last example) capture the interaction effects.
The whole model is called a ==generalized linear regression model==, and $\{ g_{s} \}_{s=1}^{S}$ are called basis functions.

Choosing indicator functions as basis functions gives ==step function regression==.
Another example of a generalized linear regression model is [[Splines]].

### Dummy Variables for Qualitative Predictors

We can use dummy variables (indicator functions) to represent qualitative predictors. One problem is, if we introduce a dummy variable for each level of a qualitative predictor, the matrix $X$ will be singular. To avoid this, we can introduce $k-1$ dummy variables for a qualitative predictor with $k$ levels. The left level can be regarded as the control/baseline/reference category.
Then, the slope of each dummy variable represents the **difference** between the corresponding level and the baseline category.
And the difference between different slopes represents the **difference** between different levels.
