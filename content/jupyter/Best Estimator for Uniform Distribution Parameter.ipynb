{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b8cab13d",
   "metadata": {},
   "source": [
    "---\n",
    "{\"publish\":true,\"title\":\"Best Estimator for Uniform Distribution Parameter\",\"created\":\"2022-12-06T00:26:27\",\"modified\":\"2025-06-18T16:39:12\",\"cssclasses\":\"\",\"aliases\":null,\"type\":\"jupyter\",\"sup\":[\"[[Probability Theory]]\",\"[[Uniform Distribution]]\"],\"state\":\"done\",\"related\":[\"[[Maximum Likelihood Estimation]]\"],\"reference\":[\"https://math.unm.edu/~knrumsey/pdfs/projects/Uniform.pdf\"]}\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb902be8",
   "metadata": {},
   "source": [
    "\n",
    "# Best Estimator for Uniform Distribution Parameter\n",
    "\n",
    "We want to estimate the parameter $\\theta$ of a [[Uniform Distribution]] given $n$ i.i.d samples $X_i \\overset{ \\text{i.i.d.} }{ \\sim } \\operatorname{Unif}[0,\\theta ]$.\n",
    "\n",
    "- [?] So, what is the *best* estimator?\n",
    "\n",
    "First, we need to define the evaluation metric. We use [[Mean Squared Error]]. We know for an estimator $\\hat{\\theta}$ of $\\theta$, its [[Mean Squared Error\\|MSE]] is\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta},\\theta ) = \\operatorname{Bias}(\\hat{\\theta})^{2} + \\operatorname{SE}(\\hat{\\theta})^{2},\n",
    "$$\n",
    "where $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$ is the bias of the estimator, and $\\operatorname{SE}(\\hat{\\theta}) = \\sqrt{\\operatorname{Var}(\\hat{\\theta})}$ is the standard error of the estimator.\n",
    "\n",
    "We will also touch on [[Evaluating an Estimator#Asymptotic Normality]] for certain estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('seaborn-v0_8-colorblind')\n",
    "\n",
    "# Set parameters for demonstrations\n",
    "np.random.seed(42)\n",
    "theta_true = 1.0  # True parameter value\n",
    "sample_sizes = [5, 10, 20, 50, 100]\n",
    "num_simulations = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf344fb",
   "metadata": {},
   "source": [
    "## First Attempts\n",
    "\n",
    "Note that the [[Expectation]] of $\\operatorname{Unif}[0,\\theta]$ is $\\theta /2$, which gives $\\theta = 2\\mathbb{E}[X]$. Therefore, the very first estimator we can think of is to replace the expectation with a sample value:\n",
    "$$\n",
    "\\hat{\\theta}^{(1)} = 2 X_{1}.\n",
    "$$\n",
    "\n",
    "We know this estimator is unbiased and thus its MSE is\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(1)}) = \\Var(2X_{1}) = 4 \\Var(X_{1}) = 4\\cdot \\frac{\\theta^{2}}{12} = \\frac{\\theta^{2}}{3},\n",
    "$$\n",
    "which is a constant regardless of the sample size $n$.\n",
    "\n",
    "Obviously, $\\hat{\\theta}^{(1)}$ is not satisfactory as it only uses the information from one sample. More generally, we can consider an estimator that uses $k \\le n$ samples:\n",
    "$$\n",
    "\\hat{\\theta}^{(k)} = \\frac{1}{k} \\sum_{i=1}^{k} 2X_{i},\n",
    "$$\n",
    "which is still unbiased but has a reduced standard error:\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(k)}) = \\Var\\left( \\frac{1}{k} \\sum_{i=1}^{k}2X_{i} \\right)  = \\frac{4}{k^{2}} \\sum_{i=1}^{k}\\Var(X_{i}) = \\frac{\\theta^{2}}{3k}.\n",
    "$$\n",
    "The variance reduces because $\\hat{\\theta}^{(k)}$ aggregates the information from $k$ i.i.d. samples.\n",
    "Again, its MSE is a constant w.r.t $n$.\n",
    "\n",
    "We plot the histogram of $\\hat{\\theta}^{(k)}$ for different $k$ values to see how the distribution changes with sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79dc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_sample_estimator(sample,k):\n",
    "    assert k <= len(sample), \"k must be less than or equal to the sample size\"\n",
    "    return 2 * np.mean(sample[:k], axis=0)\n",
    "\n",
    "n_sim = sample_sizes[2]\n",
    "samples_hist = np.random.uniform(0, theta_true, size=(n_sim, num_simulations)) # reuse for later simulations\n",
    "ks = list(range(0,n_sim,5))\n",
    "ks[0] = 1\n",
    "estmates = np.empty((len(ks), num_simulations))\n",
    "\n",
    "for kind in range(len(ks)):\n",
    "    estmates[kind, :] = k_sample_estimator(samples_hist, ks[kind])\n",
    "\n",
    "# Plot all histograms together in a plot\n",
    "plt.subplot(1, 1, 1)\n",
    "for kind in range(len(ks)):\n",
    "    sns.histplot(estmates[kind, :], stat='density', label=f'$k={ks[kind]}$', bins=20, binrange=(0,2), alpha=0.5)\n",
    "plt.axvline(theta_true, color='red', linestyle='--', label='True $\\theta$')\n",
    "plt.title(f'Histograms of $\\\\hat{{\\\\theta}}^{{(k)}}$ for different k (n={n_sim})')\n",
    "plt.xlabel('$\\\\hat{\\\\theta}^{(k)}$')\n",
    "plt.yticks([])\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243e790",
   "metadata": {},
   "source": [
    "## Method of Moments\n",
    "\n",
    "A natural extension is to use all $n$ samples:\n",
    "$$\n",
    "\\hat{\\theta }^{(\\mathrm{MM})} = \\frac{1}{n}\\sum_{i=1}^{n}2X_{i} = 2\\overline{X}.\n",
    "$$\n",
    "Since the above is equivalent to solving the estimation equation\n",
    "$$\n",
    "2\\overline{X} = \\hat{\\mathbb{E}}_{n}2X = \\mathbb{E}_{\\hat{\\theta}^{(\\mathrm{MM})}} 2X = \\hat{\\theta}^{(\\mathrm{MM})},\n",
    "$$\n",
    "the resultant estimator is a [[Method of Moments\\|Moment Estimator]]. In other words, we plug in the sample mean as the true mean (first moment) to get the estimation.\n",
    "The mean squared error is the same as the previous case with $n$ samples:\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MM})}) = \\frac{\\theta^{2}}{3n}.\n",
    "$$\n",
    "\n",
    "We compare the MSE and histogram of the moment estimator with $\\hat{\\theta}^{(2)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f659cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moment_estimator(sample):\n",
    "\t\treturn 2 * np.mean(sample, axis=0)\n",
    "\n",
    "estimators_mse = {\n",
    "    'two samples': lambda n: theta_true**2 / (3 * 2),\n",
    "    'MM': lambda n: theta_true**2 / (3 * n),\n",
    "}\n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "ax_mse = plt.subplot(2, 1, 1)\n",
    "n_mse = range(2,30)\n",
    "def plot_mse(ax,estimators_mse,n_mse):\n",
    "    for m in estimators_mse:\n",
    "        ax.plot(n_mse, [estimators_mse[m](n) for n in n_mse], label=f'{m}')\n",
    "    ax.set_xlabel('Sample Size n')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('MSE of Estimators vs Sample Size')\n",
    "    ax.legend()\n",
    "plot_mse(ax_mse, estimators_mse, n_mse)\n",
    "\n",
    "\n",
    "estimators = {\n",
    "    'two samples': lambda sample: k_sample_estimator(sample, 2),\n",
    "    'MM': moment_estimator,\n",
    "}\n",
    "\n",
    "estimates = np.empty((0, num_simulations)) # expand this array in later simulations\n",
    "for i, (name, estimator) in enumerate(estimators.items()):\n",
    "    # Check if estimates has already been filled for this method\n",
    "    if estimates.shape[0] > i:\n",
    "        continue\n",
    "    estimates = np.vstack((\n",
    "        estimates,\n",
    "        estimator(samples_hist)\n",
    "    ))\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "ax_hist = plt.subplot(2, 1, 2)\n",
    "def plot_hist(ax, estimators, estimates):  \n",
    "    for i, (name, estimator) in enumerate(estimators.items()):\n",
    "        sns.histplot(estimates[i, :], stat='density', label=name, bins=20, binrange=(0,2), alpha=0.5)\n",
    "    ax.axvline(theta_true, color='red', linestyle='--', label='$\\\\theta$')\n",
    "    ax.set_title(f'Histograms of Estimators (n={n_sim})')\n",
    "    ax.set_xlabel('$\\\\hat{\\\\theta}$')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_yticks([])\n",
    "    ax.legend()\n",
    "plot_hist(ax_hist, estimators, estimates)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe56494",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "> [!exercise] Derive the maximum likelihood estimator for $\\theta$: $\\hat{\\theta}^{(\\mathrm{MLE})}=\\max_i X_i$.\n",
    "\n",
    "To get $\\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MLE})})$ and $\\operatorname{Bias}(\\hat{\\theta}^{(\\mathrm{MLE})})$, we first need to calculate the distribution of $\\hat{\\theta}^{(\\mathrm{MLE})} = \\max_{i} X _i$.\n",
    "$$\n",
    "P(\\hat{\\theta}^{(\\mathrm{MLE})} \\le x) = P(\\max_{i} X _i \\le x) = \\prod_{i}P(X _i\\le x) = (x / \\theta)^{n}.\n",
    "$$\n",
    "\n",
    "Therefore, the [[Probability Density Function\\|PDF]] of $\\hat{\\theta}^{(\\mathrm{MLE})}$ is\n",
    "$$\n",
    "f(x) = \\frac{nx^{n-1}}{\\theta ^{n}}, \\quad x \\in [0,\\theta ]\n",
    "$$\n",
    "\n",
    "Then, we have\n",
    "$$\\tag{1}\n",
    "\\mathbb{E}[\\hat{\\theta}^{(\\mathrm{MLE})}] = \\int _{0}^{\\theta}xf(x) \\, dx = \\frac{n}{\\theta ^{n}} \\frac{1}{n+1}\\theta ^{n+1} = \\frac{n\\theta}{n+1},\n",
    "$$\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MLE})}) = \\mathbb{E}[(\\hat{\\theta}^{(\\mathrm{MLE})})^{2}] - \\mathbb{E}[\\hat{\\theta}^{(\\mathrm{MLE})}]^{2} = \\frac{n\\theta^{2}}{n+2} - \\left( \\frac{n\\theta}{n+1} \\right)^{2} = \\frac{n\\theta^{2}}{(n+2)(n+1)^{2}}.\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MLE})}) = \\frac{2\\theta^{2}}{(n+2)(n+1)} \\le \\frac{\\theta^{2}}{3n} = \\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MM})} ).\n",
    "$$\n",
    "\n",
    "Thus, we can say that $\\hat{\\theta}^{(\\mathrm{MLE})} = \\max_{i}X _i$ is a better estimator than $\\hat{\\theta}^{(\\mathrm{MM})}= 2\\overline{X}$.\n",
    "\n",
    "We compare the MLE with previous estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_estimator(sample):\n",
    "    return np.max(sample, axis=0)\n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "estimators_mse['MLE'] = lambda n: 2*theta_true**2 / ((n + 2) * (n + 1))\n",
    "plot_mse(plt.subplot(2, 1, 1), estimators_mse, n_mse)\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "estimators['MLE'] = mle_estimator\n",
    "if estimates.shape[0] < len(estimators):\n",
    "    estimates = np.vstack((estimates, mle_estimator(samples_hist)))\n",
    "else:\n",
    "    id = list(estimators.keys()).index('MLE')\n",
    "    estimates[id, :] = mle_estimator(samples_hist)\n",
    "plot_hist(plt.subplot(2, 1, 2), estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274dfe48",
   "metadata": {},
   "source": [
    "## Uniformly Minimum-Variance Unbiased Estimator\n",
    "\n",
    "Can we do better? Equation $(1)$ suggests that $\\frac{n+1}{n}\\hat{\\theta}^{(\\mathrm{MLE})}$ is an unbiased estimator, which may further reduce the error.\n",
    "\n",
    "We have the following fact:\n",
    "\n",
    "> [!prop] Proposition ^prop\n",
    ">\n",
    "> If $T$ is a complete and [[Sufficient Statistic]] for a parameter $\\theta$, and $\\phi(T)$ is an estimator dependent only on $T$, then $\\phi(T)$ is the unique uniformly minimum-variance unbiased estimator (UMVUE) of $\\mathbb{E}_{\\theta }\\phi(T)$.\n",
    "\n",
    "The uniformness refers to that the minimum variance is achieved for all $\\theta$.\n",
    "\n",
    "We note that $\\max_{i}X_{i}$ is a complete and sufficient statistic for $\\theta$. Thus, the estimator\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{UMVUE})} = \\frac{n+1}{n}\\max_{i}X_{i},\n",
    "$$\n",
    "has the minimum variance among all unbiased estimators of $\\theta$.\n",
    "\n",
    "We calculate its variance, i.e., MSE:\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{UMVUE})}) = \\operatorname{Var}\\left( \\frac{n+1}{n}\\hat{\\theta}^{(\\mathrm{MLE})} \\right) = \\left( \\frac{n+1}{n} \\right)^{2}\\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MLE})}) = \\frac{\\theta^{2}}{n(n+2)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad1395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umvue_estimator(sample):\n",
    "    n = len(sample)\n",
    "    return (n+1) / n * np.max(sample, axis=0)\n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "estimators_mse['UMVUE'] = lambda n: theta_true**2 / (n * (n + 2))\n",
    "plot_mse(plt.subplot(2, 1, 1), estimators_mse, n_mse)\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "estimators['UMVUE'] = umvue_estimator\n",
    "if estimates.shape[0] < len(estimators):\n",
    "    estimates = np.vstack((estimates, umvue_estimator(samples_hist)))\n",
    "else:\n",
    "    id = list(estimators.keys()).index('UMVUE')\n",
    "    estimates[id, :] = umvue_estimator(samples_hist)\n",
    "plot_hist(plt.subplot(2, 1, 2), estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f412b8",
   "metadata": {},
   "source": [
    "## Jackknife\n",
    "\n",
    "In our problem, the bias of the MLE can be explicitly calculated, and thus we can directly correct it using the [[Best Estimator for Uniform Distribution Parameter#Uniformly Minimum-Variance Unbiased Estimator\\|UMVUE]]. For more general biased estimators, we can use Jackknife resampling to *estimate* the bias, and then correct the estimator.\n",
    "\n",
    "The first step of this procedure is to produce a series of [[Cross-Validation\\|leave-one-out]] estimates:\n",
    "$$\n",
    "\\hat{\\theta}_{(-i)} = \\hat{\\theta}_{\\{ X_{1},\\dots,X_{i-1},X_{i+1},\\dots,X_n \\}}.\n",
    "$$\n",
    "That is, we remove one sample $X_{i}$ and construct the estimator $\\hat{\\theta}_{(-i)}$ using the remaining samples.\n",
    "Then, the Jackknife bias estimate is\n",
    "$$\n",
    "\\widehat{\\operatorname{Bias}}(\\hat{\\theta}_{n}) = (n-1)\\left( \\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\theta}_{(-i)} - \\hat{\\theta}_{n} \\right) ,\n",
    "$$\n",
    "where $\\hat{\\theta}_{n}$ is the original estimator (with $n$ samples) to be corrected.\n",
    "Thus, the corrected Jackknife estimator is\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{Jack})} = \\hat{\\theta} - \\widehat{\\operatorname{Bias}}(\\hat{\\theta}_{n}) = n\\hat{\\theta}_{n} -  \\frac{n-1}{n}\\sum_{i=1}^{n}\\hat{\\theta}_{(-i)}.\n",
    "$$\n",
    "\n",
    "Generally, the MSE of a Jackknife estimator is difficult to calculate as $\\hat{\\theta}_{(-i)}$ are correlated. However, if we are to correct the MLE estimator for $\\theta$, the Jackknife estimator has a simple form:\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{Jack})} = \\frac{2n-1}{n} X_{(n)} - \\frac{n-1}{n}X_{(n-1)},\n",
    "$$\n",
    "where $X_{(i)}$ is the $i$-th order statistic of the sample $X_{1},\\dots,X_{n}$, and thus $X_{(n)}=\\max_{i}X_{i}$.\n",
    "Moreover, we can calculate its MSE, which slightly improves that of the MLE estimator:\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{Jack})}) = \\left( 1- \\frac{n-1}{n^{2}} \\right) \\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MLE})}).\n",
    "$$\n",
    "\n",
    "See [[Best Estimator for Uniform Distribution Parameter#Appendix]] for details of the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jackknife_estimator(sample):\n",
    "    mle = mle_estimator(sample)\n",
    "    # Produce leave-one-out MLEs\n",
    "    mle_loo = 0\n",
    "    n = sample.shape[0]\n",
    "    for i in range(n):\n",
    "        mle_loo = mle_loo + mle_estimator(np.delete(sample, i, axis=0))\n",
    "    return n * mle - (n - 1) / n * mle_loo \n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "estimators_mse['Jackknife'] = lambda n: 2 * (n**2 - n + 1) * theta_true**2 / (n**2 * (n + 1) * (n + 2))\n",
    "plot_mse(plt.subplot(2, 1, 1), estimators_mse, n_mse)\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "estimators['Jackknife'] = jackknife_estimator\n",
    "if estimates.shape[0] < len(estimators):\n",
    "    estimates = np.vstack((estimates, jackknife_estimator(samples_hist)))\n",
    "else:\n",
    "    id = list(estimators.keys()).index('Jackknife')\n",
    "    estimates[id, :] = jackknife_estimator(samples_hist)\n",
    "plot_hist(plt.subplot(2, 1, 2), estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca0f4f",
   "metadata": {},
   "source": [
    "## Minimal MSE\n",
    "\n",
    "When we look at the MSE, for [[Best Estimator for Uniform Distribution Parameter#Maximum Likelihood Estimation\\|MLE]], bias dominates, while for [[Best Estimator for Uniform Distribution Parameter#Uniformly Minimum-Variance Unbiased Estimator\\|UMVUE]] and [[Best Estimator for Uniform Distribution Parameter#Jackknife]], variance dominates.\n",
    "A natural next step is to find the estimator that achieves the optimal balance between [[Bias-Variance Trade-Off\\|bias and variance]]. Actually, such an estimator is indeed the *best* estimator for $\\theta$ in terms of MSE (see [[Best Estimator for Uniform Distribution Parameter#Appendix]]).\n",
    "\n",
    "Consider a general form of the MLE and UMVUE using the complete and sufficient statistic $X _{(n)}$:\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{MMSE})} = cX_{(n)}\n",
    "$$\n",
    "$c=1$ recovers the MLE and $c=(n+1)/n$ recovers the UMVUE.\n",
    "For a general $c$, we have\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\theta}^{(\\mathrm{MMSE})}] = \\frac{cn\\theta}{n+1},\n",
    "\\quad\n",
    "\\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MMSE})}) = \\frac{c^{2}n\\theta^{2}}{(n+1)^{2}(n+2)}.\n",
    "$$\n",
    "Thus,\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MMSE})}) = \\frac{\\theta^{2}}{(n+1)^{2}(n+2)} (\\underbrace{c^{2}n + (n+1-cn)^{2}(n+2)}_{f(c)}) \n",
    "$$\n",
    "Setting\n",
    "$$\n",
    "\\frac{ \\mathrm{d}f }{ \\mathrm{d}c } = 2nc - 2n(n+2)(n+1-cn) = 0\n",
    "$$\n",
    "gives $c = \\frac{n+2}{n+1}$. Therefore, we get\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{MMSE})} = \\frac{n+2}{n+1} \\max_{i}X_{i},\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MMSE})}) = \\frac{\\theta^{2}}{(n+1)^{2}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3620cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmse_estimator(sample):\n",
    "    n = sample.shape[0]\n",
    "    return (n + 2) / (n + 1) * np.max(sample, axis=0)\n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "estimators_mse['MMSE'] = lambda n: theta_true**2 / ((n + 1) ** 2)\n",
    "plot_mse(plt.subplot(2, 1, 1), estimators_mse, n_mse)\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "estimators['MMSE'] = mmse_estimator\n",
    "if estimates.shape[0] < len(estimators):\n",
    "    estimates = np.vstack((estimates, mmse_estimator(samples_hist)))\n",
    "else:\n",
    "    id = list(estimators.keys()).index('MMSE')\n",
    "    estimates[id, :] = mmse_estimator(samples_hist)\n",
    "plot_hist(plt.subplot(2, 1, 2), estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320db921",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The following table summarizes the estimators we have discussed.\n",
    "\n",
    "| Estimator         | Expression                                         | Bias                        | Variance                                   | MSE                                            |\n",
    "| ----------------- | -------------------------------------------------- | --------------------------- | ------------------------------------------ | ---------------------------------------------- |\n",
    "| $k$ Samples       | $\\frac{2}{k} \\sum_{i=1}^{k} X_i$                   | $0$                         | $\\frac{\\theta^2}{3k}$                      | $\\frac{\\theta^2}{3k}$                          |\n",
    "| Method of Moments | $2\\overline{X}$                                    | $0$                         | $\\frac{\\theta^2}{3n}$                      | $\\frac{\\theta^2}{3n}$                          |\n",
    "| MLE               | $X_{(n)}$                                          | $-\\frac{\\theta}{n+1}$       | $\\frac{n\\theta^2}{(n+2)(n+1)^2}$           | $\\frac{2\\theta^2}{(n+2)(n+1)}$                 |\n",
    "| UMVUE             | $\\frac{n+1}{n} X_{(n)}$                            | $0$                         | $\\frac{\\theta^2}{n(n+2)}$                  | $\\frac{\\theta^2}{n(n+2)}$                      |\n",
    "| Jackknife         | $\\frac{2n-1}{n} X_{(n)} - \\frac{n-1}{n} X_{(n-1)}$ | $-\\frac{\\theta}{n(n+1)}$    | $\\frac{(2n^2 - 1)\\theta^2}{n(n+1)^2(n+2)}$ | $\\frac{2(n^2 - n + 1)\\theta^2}{n^2(n+1)(n+2)}$ |\n",
    "| MMSE              | $\\frac{n+2}{n+1} X_{(n)}$                          | $-\\frac{\\theta}{(n+1)^{2}}$ | $\\frac{n(n+2)\\theta^2}{(n+1)^4}$           | $\\frac{\\theta^2}{(n+1)^2}$                     |\n",
    "\n",
    "Finally, we plot the histograms of all estimators separately to compare their distributions, and calculate their empirical mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of all estimators separately\n",
    "num_bins = 20\n",
    "range_bins = (0.8, 1.2)\n",
    "y_lim_high = 0\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"]()\n",
    "\n",
    "fig, ax = plt.subplots(round(len(estimators) / 2), 2)\n",
    "for i, (name, estimator) in enumerate(estimators.items()):\n",
    "    plt.subplot(round(len(estimators) / 2), 2, i + 1)\n",
    "    # Match the color increment\n",
    "    sns.histplot(estimates[i, :], stat='density', bins=num_bins, binrange=range_bins, alpha=0.5, color=next(colors)['color'])\n",
    "    plt.axvline(theta_true, color='red', linestyle='--', label='$\\\\theta$')\n",
    "    plt.title(f'{name}')\n",
    "    # log the y lim\n",
    "    y_lim_high = max(y_lim_high, plt.ylim()[1])\n",
    "    \n",
    "plt.setp(ax, ylim=(0, y_lim_high))\n",
    "fig.supxlabel('$\\\\hat{\\\\theta}$')\n",
    "fig.supylabel('Density')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate empirical MSE for each estimator\n",
    "num_simulations = int(1e3)\n",
    "mse_empirical = np.empty((len(estimators), len(sample_sizes), num_simulations))\n",
    "mse_empirical[:] = np.nan\n",
    "for n in sample_sizes:\n",
    "    samples = np.random.uniform(0, theta_true, size=(n, num_simulations))\n",
    "    estimates = np.empty((num_simulations, len(estimators)))\n",
    "    for i, (name, estimator) in enumerate(estimators.items()):\n",
    "        mse_empirical[i, sample_sizes.index(n), :] = (estimator(samples) - theta_true)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot empirical MSE with 0.95 confidence region using fill_between for each estimator in a single plot\n",
    "plt.figure()\n",
    "for i, (name, estimator) in enumerate(estimators.items()):\n",
    "    mean_mse = np.mean(mse_empirical[i, :, :], axis=1)\n",
    "    std_mse = np.std(mse_empirical[i, :, :], axis=1)\n",
    "    err_high = mean_mse + 1.96 * std_mse / np.sqrt(num_simulations)\n",
    "    err_low = mean_mse - 1.96 * std_mse / np.sqrt(num_simulations)\n",
    "\n",
    "    plt.plot(sample_sizes, mean_mse, label=name)\n",
    "    plt.fill_between(sample_sizes, err_low, err_high, alpha=0.2)\n",
    "    # Remove legend handles for the fill_between\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Sample Size n')\n",
    "plt.ylabel('Empirical MSE')\n",
    "plt.title('Empirical MSE of Estimators vs Sample Size')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2d70c",
   "metadata": {},
   "source": [
    "## Beyond MSE\n",
    "\n",
    "So far, we have focused on the MSE as the metric for evaluating estimators. In this section, we first explore a new risk, and then discuss the statistical properties of MLE (and thus other estimators built on $\\max_i X_i$) for the uniform distribution.\n",
    "\n",
    "### Zero-One Loss\n",
    "\n",
    "Recall that MSE is the risk associated with the squared loss $L(\\hat{\\theta},\\theta) = (\\hat{\\theta} - \\theta)^{2}$.\n",
    "\n",
    "Consider a new loss function:\n",
    "$$\n",
    "L(\\hat{\\theta},\\theta) = \\mathbb{1}\\left\\{ |\\hat{\\theta}-\\theta| > \\epsilon \\right\\}.\n",
    "$$\n",
    "This zero-one loss function is often used in binary decision-making problems. In the context of parameter estimation, it finds applications in catastrophic risk assessment, where any estimation error beyond a certain threshold $\\epsilon$ is considered catastrophic. Then, the corresponding risk\n",
    "$$\n",
    "R(\\hat{\\theta},\\theta) = \\mathbb{E}_{\\theta}L(\\hat{\\theta},\\theta) = P_{\\theta}\\left( |\\hat{\\theta}-\\theta| > \\epsilon \\right)\n",
    "$$\n",
    "is the probability of catastrophe.\n",
    "\n",
    "Again, let's consider an estimator of the general form $\\hat{\\theta}^{(\\mathrm{ZO})} = cX_{(n)}$, and try to minimize the zero-one risk. We have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  R(\\hat{\\theta}^{(\\mathrm{ZO})} ,\\theta) =& P_{\\theta}\\left( |cX_{(n)} - \\theta| > \\epsilon \\right) \\\\\n",
    "  =& P_{\\theta}\\left( cX_{(n)} - \\theta > \\epsilon \\right) + P_{\\theta}\\left( cX_{(n)} - \\theta < -\\epsilon \\right) \\\\\n",
    "  =& P_{\\theta}\\left( X_{(n)} >c^{-1} (\\theta + \\epsilon)\\right) + P_{\\theta}\\left( X_{(n)} < c^{-1} (\\theta - \\epsilon)\\right) \\\\\n",
    "  =& 1-F_{X_{(n)}}\\left( c^{-1} (\\theta + \\epsilon) \\right) + F_{X_{(n)}}\\left( c^{-1} (\\theta - \\epsilon) \\right)\n",
    ".\\end{aligned}\n",
    "$$\n",
    "We have already derived the CDF of $X_{(n)}$ in [[Best Estimator for Uniform Distribution Parameter#Maximum Likelihood Estimation]], which gives\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R(\\hat{\\theta}^{(\\mathrm{ZO})} ,\\theta) =& \\begin{cases}\n",
    "1 - \\left( \\frac{c^{-1} (\\theta + \\epsilon)}{\\theta} \\right)^{n} + \\left( \\frac{c^{-1} (\\theta - \\epsilon)}{\\theta} \\right)^{n}\n",
    ", \\quad &1 + \\epsilon /\\theta < c\\\\\n",
    "\\left( \\frac{c^{-1} (\\theta - \\epsilon)}{\\theta} \\right)^{n}\n",
    "= c^{-n}(1- \\epsilon/\\theta)^{n} , \\quad & 1 - \\epsilon /\\theta \\le c \\le 1 + \\epsilon /\\theta \\\\\n",
    "1, \\quad & 1 - \\epsilon /\\theta > c  \\\\\n",
    "\\end{cases}\\\\\n",
    "=& \\begin{cases}\n",
    "1 - c^{-n} \\left( (1+ \\epsilon/\\theta)^{n} - (1- \\epsilon/\\theta)^{n} \\right), \\quad & 1 + \\epsilon /\\theta < c \\\\\n",
    "c^{-n}(1- \\epsilon/\\theta)^{n} , \\quad & 1 - \\epsilon /\\theta \\le c \\le 1 + \\epsilon /\\theta \\\\\n",
    "1, \\quad & 1 - \\epsilon /\\theta > c  \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "For a fixed $\\theta$, we usually consider a small threshold $\\epsilon$. In such scenarios,\n",
    "$$\n",
    "(1+\\epsilon /\\theta)^{n} \\approx 1 + n\\epsilon /\\theta, \\quad \\text{and} \\quad (1-\\epsilon /\\theta)^{n} \\approx 1 - n\\epsilon /\\theta.\n",
    "$$\n",
    "Using the above approximation, one can easily verify that $c = 1 + \\epsilon /\\theta$ minimizes the zero-one risk.\n",
    "\n",
    "Note that the estimator should not depend on the true parameter $\\theta$. Thus, we consider a zero-one loss based on the relative distance:\n",
    "$$\n",
    "R(\\hat{\\theta},\\theta) = P_{\\theta}\\left( |\\hat{\\theta} - \\theta| > \\delta  \\theta \\right) ,\n",
    "$$\n",
    "whose corresponding optimal estimator is thus\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{ZO})} = \\left( 1 + \\delta \\right) X_{(n)}.\n",
    "$$\n",
    "\n",
    "We plot the histogram of the zero-one estimator for different $\\delta$ values, and compare it with the MMSE estimator on both the MSE and zero-one risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one_estimator(sample, delta):\n",
    "    return (1 + delta) * np.max(sample, axis=0)\n",
    "\n",
    "# Plot histograms of the zero-one estimator for different delta values\n",
    "delta_values = [0.05, 0.1, 0.2, 0.5]\n",
    "num_simulations = 100000\n",
    "estimates_zo = np.empty((len(delta_values), num_simulations))\n",
    "for i, delta in enumerate(delta_values):\n",
    "    estimates_zo[i, :] = zero_one_estimator(samples_hist, delta)\n",
    "\n",
    "plt.figure()\n",
    "for i, delta in enumerate(delta_values):\n",
    "    sns.histplot(estimates_zo[i, :], stat='density', label=f'$\\\\delta={delta}$', bins=20, binrange=(0.8,1.6), alpha=0.5)\n",
    "# Plot MLE estimator for comparison\n",
    "mle_estimates = mle_estimator(samples_hist)\n",
    "sns.histplot(mle_estimates, stat='density', label='MLE', bins=20, binrange=(0.8,1.6), alpha=0.5)\n",
    "plt.axvline(theta_true, color='red', linestyle='--', label='True $\\\\theta$')\n",
    "plt.title('Histograms of Zero-One Estimator for Different $\\\\delta$ Values')\n",
    "plt.xlabel('$\\\\hat{\\\\theta}$')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db22580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the empirical risks and zero-one risk for the zero-one estimator and MMSE\n",
    "num_simulations = int(1e3)\n",
    "delta = 5e-3\n",
    "risk_empirical = np.empty((2, 2, len(sample_sizes), num_simulations)) # 2 risks (MSE and zero-one), 2 estimators (MMSE and zero-one), sample sizes, simulations\n",
    "risk_empirical[:] = np.nan\n",
    "for n in sample_sizes:\n",
    "    samples = np.random.uniform(0, theta_true, size=(n, num_simulations))\n",
    "    # Minimum MSE Estimator\n",
    "    estimates_mmse = mmse_estimator(samples)\n",
    "    risk_empirical[0, 0, sample_sizes.index(n), :] = (estimates_mmse - theta_true)**2\n",
    "    risk_empirical[1, 0, sample_sizes.index(n), :] = np.abs(estimates_mmse - theta_true) > delta * theta_true\n",
    "    # Zero-One Estimator\n",
    "    estimates_zo = zero_one_estimator(samples, delta)\n",
    "    risk_empirical[0, 1, sample_sizes.index(n), :] = (estimates_zo - theta_true)**2\n",
    "    risk_empirical[1, 1, sample_sizes.index(n), :] = np.abs(estimates_zo - theta_true) > delta * theta_true\n",
    "\n",
    "# Plot empirical MSE for the zero-one estimator and MMSE\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "mean_mse_mmse = np.mean(risk_empirical[0, 0, :, :], axis=1)\n",
    "ci_mse_mmse = 1.96 * np.std(risk_empirical[0, 0, :, :], axis=1) / np.sqrt(num_simulations)\n",
    "plt.plot(sample_sizes, mean_mse_mmse, label='MMSE')\n",
    "plt.fill_between(sample_sizes, mean_mse_mmse - ci_mse_mmse, mean_mse_mmse + ci_mse_mmse, alpha=0.2)\n",
    "\n",
    "mean_mse_zo = np.mean(risk_empirical[0, 1, :, :], axis=1)\n",
    "ci_mse_zo = 1.96 * np.std(risk_empirical[0, 1, :, :], axis=1) / np.sqrt(num_simulations)\n",
    "plt.plot(sample_sizes, mean_mse_zo, label='Zero-One')\n",
    "plt.fill_between(sample_sizes, mean_mse_zo - ci_mse_zo, mean_mse_zo + ci_mse_zo, alpha=0.2)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Empirical MSE')\n",
    "\n",
    "# Plot empirical zero-one risk for the zero-one estimator and MMSE\n",
    "plt.subplot(1, 2, 2)\n",
    "mean_risk_mmse = np.mean(risk_empirical[1, 0, :, :], axis=1)\n",
    "ci_risk_mmse = 1.96 * np.std(risk_empirical[1, 0, :, :], axis=1) / np.sqrt(num_simulations)\n",
    "plt.plot(sample_sizes, mean_risk_mmse, label='MMSE')\n",
    "plt.fill_between(sample_sizes, mean_risk_mmse - ci_risk_mmse, mean_risk_mmse + ci_risk_mmse, alpha=0.2)\n",
    "mean_risk_zo = np.mean(risk_empirical[1, 1, :, :], axis=1)\n",
    "ci_risk_zo = 1.96 * np.std(risk_empirical[1, 1, :, :], axis=1) / np.sqrt(num_simulations)\n",
    "plt.plot(sample_sizes, mean_risk_zo, label='Zero-One')\n",
    "plt.fill_between(sample_sizes, mean_risk_zo - ci_risk_zo, mean_risk_zo + ci_risk_zo, alpha=0.2)\n",
    "plt.title('Empirical Zero-One Risk')\n",
    "# plt.xlim(sample_sizes[0], sample_sizes[-2])\n",
    "\n",
    "plt.setp(fig.axes, yscale='log')\n",
    "fig.supxlabel('Sample Size n')\n",
    "fig.supylabel('Risk')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc50397",
   "metadata": {},
   "source": [
    "### Statistical Properties of MLE for Uniform Distribution\n",
    "\n",
    "[[Maximum Likelihood Estimation\\|MLE]] is known to be the *best* estimator in terms of statistical properties, such as consistency and asymptotic normality, under mild conditions. However, in this note, we have shown that the MLE for the uniform distribution is biased and has a larger MSE than some other estimators. Do our findings contradict the properties of MLE?\n",
    "\n",
    "We first verify the consistency. In the previous section, we show that for $c=1$ and any $\\epsilon\\in(0,\\theta)$,\n",
    "$$\n",
    "P_{\\theta}(|\\hat{\\theta}^{(\\mathrm{MLE})} - \\theta| > \\epsilon) = (1-\\epsilon /\\theta)^{n}\\to 0.\n",
    "$$\n",
    "Thus, $\\hat{\\theta}^{(\\mathrm{MLE})}$ is consistent.\n",
    "\n",
    "For asymptotic normality, we notice that\n",
    "$$\n",
    "\\operatorname{supp}(\\sqrt{n}(\\hat{\\theta}^{(\\mathrm{MLE})} - \\theta)) = ( -\\sqrt{n}\\theta,0) \\to (-\\infty,0) \\ne \\mathbb{R}.\n",
    "$$\n",
    "Thus, $\\hat{\\theta}^{(\\mathrm{MLE})}$ cannot be asymptotically normal.\n",
    "Specifically, the uniform distribution fails to meet the regularity condition that the support of the distribution should not depend on the parameter $\\theta$.\n",
    "\n",
    "Moreover, we actually have\n",
    "$$\n",
    "P_{\\theta}(n(\\theta-\\hat{\\theta}^{(\\mathrm{MLE})} ) \\le t) = 1 - P_{\\theta}(\\hat{\\theta}^{(MLE)} \\le \\theta - t/n) = 1 - (1 - t /(n\\theta))^{n} \\to 1-e^{-t /\\theta},\n",
    "$$\n",
    "indicating that\n",
    "$$\n",
    "n (\\theta-\\hat{\\theta}^{(\\mathrm{MLE})} ) \\overset{d}{\\to} \\mathrm{Exp}(1 /\\theta).\n",
    "$$\n",
    "Note that the exponential tail bound is significantly heavier than the Gaussian tail bound ($e^{-nt}$ vs. $e^{-nt^{2}}$). We verify this by simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "samples = np.random.uniform(0, theta_true, size=(n, 10000))\n",
    "mle_estimates = mle_estimator(samples)\n",
    "asymptotic_distribution = n*(theta_true - mle_estimates)\n",
    "plt.figure()\n",
    "sns.histplot(asymptotic_distribution, stat='density', alpha=0.5, label='empirical')\n",
    "x = np.linspace(0, plt.xlim()[1], 1000)\n",
    "plt.plot(x, (1/theta_true) * np.exp(-x/theta_true), label='Exp(1)', color='red', linestyle='--')\n",
    "# Plot theoretical standard normal distribution for comparison\n",
    "plt.plot(x, (1/(np.sqrt(2 * np.pi))) * np.exp(-(x)**2 / (2 )), label='Normal', color='green', linestyle='--')\n",
    "plt.title(f'Empirical distribution of $n(\\\\theta - \\\\hat{{\\\\theta}}^{{(MLE)}})$ with $n={n}$')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e52e6",
   "metadata": {},
   "source": [
    "Finally, we remark that either MSE or asymptotic normality is just one of many criteria for evaluating estimators. One estimator with smaller MSE may underestimate other risks. One asymptotically normal estimator may have larger MSE than another estimator. At the end of the day, we should choose the estimator that best fits our specific problem and risk criteria.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### Calculation of Jackknife MSE\n",
    "\n",
    "For correcting the MLE $X_{(n)}$, the Jackknife estimator has a simple form:\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{Jack})} = n X_{(n)} - \\frac{n-1}{n}\\left( (n-1)X_{(n)} + X_{(n-1)} \\right) \n",
    "= \\frac{2n-1}{n} X_{(n)} - \\frac{n-1}{n}X_{(n-1)},\n",
    "$$\n",
    "where $X_{(i)}$ is the $i$-th order statistic of the sample $X_{1},\\dots,X_{n}$, and thus $X_{(n)}=\\max_{i}X_{i}$.\n",
    "\n",
    "Similar to the calculation in [[Best Estimator for Uniform Distribution Parameter#Maximum Likelihood Estimation]] (see also [[Order Statistics#Distribution of i-th Order Statistic]]), the PDF of $X_{(n-1)}$ is\n",
    "$$\n",
    "f_{(n-1)}(x) = \\frac{n!}{(n-2)!} \\frac{(x /\\theta)^{n-2}(1-x /\\theta)}{\\theta} = \\frac{n(n-1)x^{n-2}(1-x /\\theta)}{\\theta ^{n-1}}.\n",
    "$$\n",
    "Then, we can calculate the bias:\n",
    "$$\n",
    "\\operatorname{Bias}(\\hat{\\theta}^{(\\mathrm{Jack})}) =\\frac{2n-1}{n} \\frac{n\\theta}{n+1} - \\frac{n-1}{n} \\frac{n(n-1)}{\\theta ^{n-1}}\\left( \\frac{\\theta ^{n}}{n} - \\frac{\\theta ^{n+1}}{(n+1)\\theta } \\right) -\\theta  = -\\frac{1}{n^{2}+n}\\theta .\n",
    "$$\n",
    "We can see that compared to the [[Best Estimator for Uniform Distribution Parameter#Maximum Likelihood Estimation]], the bias is significantly reduced.\n",
    "\n",
    "To calculate the variance of $\\hat{\\theta}^{(\\mathrm{Jack})}$, we need to know the joint distribution of $X_{(n)}$ and $X_{(n-1)}$. Note that the [[Order Statistics#Joint Distribution\\|joint PDF of the entire order statistic]] is given by\n",
    "$$\n",
    "f_{(X_{(i)})} (x) = \\frac{n!}{\\theta ^{n}} \\mathbb{1}\\{ x_{1}\\le \\dots\\le x_{n} \\}.\n",
    "$$\n",
    "Integrating out $(X_{(1)},\\dots ,X_{(n-2)})$ gives\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{(X_{(n-1)},X_{(n)})}(x_{n-1},x_{n}) =&\\frac{n!}{\\theta ^{n}}\\int _{0}^{x_{n-1}}\\int _{0}^{x_{n-2}}\\cdots\\int _{0}^{x_{2}} \\d x_{1} \\cdots  \\, \\d x_{n-3} \\d x_{n-2} \\\\\n",
    "=&\\frac{n!}{\\theta ^{n}}\\int _{0}^{x_{n-1} }\\int _{0}^{x_{n-2}}\\frac{1}{(n-4)!}x_{n-3}^{n-4} \\d x_{n-3}  \\d x_{n-2} \\\\\n",
    "=&n(n-1) \\frac{x_{n-1}^{n-2}}{\\theta ^{n}}, \\quad 0 \\le x_{n-1} \\le x_{n} \\le \\theta.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Thus, their covariance is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Cov(X_{(n-1)},X_{(n)}) =& \\mathbb{E}[X_{(n-1)}X_{(n)}] - \\mathbb{E}[X_{(n-1)}]\\mathbb{E}[X_{(n)}]\\\\\n",
    "=&\\int_{0}^{\\theta}\\int_{0}^{y} x y f_{(X_{(n-1)},X_{(n)})}(x,y) \\mathrm{d}x \\mathrm{d}y - \\frac{n\\theta}{n+1}  \\frac{(n-1)\\theta}{n+1}\\\\\n",
    "=& \\frac{n(n-1)}{\\theta^{n}} \\frac{\\theta^{n+2}}{n(n+2)} - \\frac{n\\theta}{n+1}  \\frac{(n-1)\\theta}{n+1} \\\\\n",
    "=& \\frac{(n-1)\\theta^{2}}{(n+1)^{2}(n+2)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Then, we can calculate the variance:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Var(\\hat{\\theta}^{(\\mathrm{Jack})}) =& \\frac{(2n-1)^{2}}{n^{2}}\\Var(X_{(n)}) + \\frac{(n-1)^{2}}{n^{2}}\\Var(X_{(n-1)}) - 2 \\frac{(2n-1)(n-1)}{n^{2}}\\Cov(X_{(n)},X_{(n-1)}) \\\\\n",
    "=&  \\frac{(2n-1)^{2}}{n^{2}}\\frac{n\\theta^{2}}{(n+2)(n+1)^{2}} \n",
    "+ \\frac{(n-1)^{2}}{n^{2}}\\frac{2(n-1)\\theta^{2}}{(n+1)^{2}(n+2)} \n",
    "- 2 \\frac{(2n-1)(n-1)}{n^{2}}\\frac{(n-1)\\theta^{2}}{(n+1)^{2}(n+2)} \\\\\n",
    "=& \\frac{\\theta^{2}}{(n+2)(n+1)^{2}} \\left( \\frac{(2n-1)^{2}}{n} + \\frac{2(n-1)^{3}}{n^{2}} - \\frac{2(2n-1)(n-1)^{2}}{n^{2}}\\right) \\\\\n",
    "=& \\frac{(2n^{2}-1)\\theta^{2}}{n(n+1)^{2}(n+2)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, we get\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{Jack})}) = \\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{Jack})}) + \\operatorname{Bias}(\\hat{\\theta}^{(\\mathrm{Jack})})^{2} = \\frac{2(n^{3}+1)\\theta^{2}}{n^{2}(n+1)^{2}(n+2)} = \\frac{2(n^{2}-n+1)}{n^{2}(n+1)(n+2)} \\theta^{2} .\n",
    "$$\n",
    "\n",
    "### Optimality of MMSE\n",
    "\n",
    "For any estimator $\\hat{\\theta}$, we write $\\mathbb{E}\\hat{\\theta}_{X} = f_n(\\theta)$. Suppose $f$ is linear, then by the linearity of expectation, we have\n",
    "$$\n",
    "\\mathbb{E}f_{n}\\left( \\frac{n+1}{n}X_{(n)} \\right) = f_{n} \\left( \\frac{n+1}{n} \\mathbb{E}X_{(n)} \\right)  = f_{n}(\\theta ) = \\mathbb{E}\\hat{\\theta}_{X}.\n",
    "$$\n",
    "Let $\\phi(T) = f_{n}\\left( \\frac{n+1}{n}T \\right)$. By Proposition [[Best Estimator for Uniform Distribution Parameter#^prop]], we know $\\phi(X_{(n)})$ has the same bias as but smaller variance than $\\hat{\\theta}_{X}$ if $\\hat{\\theta}_{X}\\ne \\phi(T)$ (uniqueness). Further, among the class of estimators consisting of linear functions of $X_{(n)}$, it is easy to see $\\hat{\\theta}^{(\\mathrm{MMSE})}$ has the smallest MSE.\n",
    "\n",
    "Now suppose $f_{n}$ is not linear. By Taylor expansion,\n",
    "$$\n",
    "f_{n}(\\theta) = \\sum_{k=0}^{\\infty} \\frac{f_n^{(k)}(0)}{k!} \\theta^k.\n",
    "$$\n",
    "For $\\hat{\\theta}$ to be uniformly optimal for any $\\theta$, it must satisfy\n",
    "$$\n",
    "f_n(0) -\\theta  = o(\\theta) \\text{ as }\\theta \\to 0  \\quad\\text{and}\\quad f_{n}(\\theta) -\\theta  = O(\\theta) \\text{ as } \\theta\\to \\infty.\n",
    "$$\n",
    "Thus, $f_{n}^{(k)} = 0$ for $k\\ne 1$ and any $n$, indicating that $f_{n}$ must be linear in $\\theta$."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
