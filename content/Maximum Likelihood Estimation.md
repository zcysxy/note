---
{"publish":true,"title":"Maximum Likelihood Estimation","created":"2022-05-28T03:34:23","modified":"2025-06-02T20:20:22","cssclasses":""}
---


# Maximum Likelihood Estimation



Maximum Likelihood Estimation (MLE) offers a principled way to estimate the parameters of a statistical model by maximizing the likelihood function. It is general and has nice [[Maximum Likelihood Estimation#Properties of ML\|properties]].

For a set of $m$ examples $X = \{x^i\}_{i=1}^m$ generated by data generating distribution $p_{\mathrm{data}}(x)$. And $p_{\mathrm{model}}(x;\theta)$ is a parametric family of model distributions estimating $p_{\mathrm{data}}$. We define the ==maximum likelihood estimator== for $\theta$

$$
\theta_{\mathrm{ML}} \triangleq \argmax_\theta p_{\mathrm{model}}(X;\theta)
= \argmax_\theta \prod_{i=1}^m p_{\mathrm{model}}(x^i;\theta)
$$

> [!rmk]
> - Here $p(x) = p(\xi = x)$
> - The latter equality requires the i.i.d. condition

Intuitively, since $X$ is generated by $p_{\mathrm{data}}$, $p_{\mathrm{data}}(x^i)$ should be relatively high. Then if $p_{\mathrm{model}}(x;\theta)$ is close enough to $p_{\mathrm{data}}$, $\prod p_{\mathrm{model}}(x^i;\theta)$ should be relatively high too.
In the same spirit, $p_{\mathrm{model}}(x;\theta)$ is called the ==likelihood function==, or written as
$$
L_{n}: \mathcal{X}^{n} \times \Theta \to \mathbb{R}, \quad (x_{1},\dots, x_n; \theta) \mapsto p_{\theta}(X_{1}=x_{1},\dots,X_n=x_n),
$$
where $p_{\theta}$ can be a [[Probability Density Function\|PDF]] or a [[Probability Mass Function\|PMF]].

## Equivalences

In practice, we often calculate the MLE by the following equivalences

$$\begin{aligned}
    \theta_{ML} &= \argmax_\theta \prod_{i=1}^m p_{\mathrm{model}}(x^i;\theta)\\
    &= \argmax_\theta \sum_{i=1}^m \log p_{\mathrm{model}}(x^i;\theta)\\
    &= \argmax_\theta \mathbb{E}_{x\sim \hat{p}_{\mathrm{data}}} \log p_{\mathrm{model}}(x;\theta)
\end{aligned}$$

where $\hat{p}_{\mathrm{data}}$ is the empirical distribution defined by the training data. This is called the ==logarithm trick==.

## Relation With KL Divergence

Another interpretation of MLE is that it minimizes the [[KL Divergence\|KL Divergence]], or [[Cross-Entropy\|Cross-Entropy]], between $p_{\mathrm{data}}$ and $p_{\mathrm{model}}$, which measures the **similarity** between the two distributions:
$$
\begin{cases}
    D_{\mathrm{KL}}(\hat{p}_{\mathrm{data}}\|p_{\mathrm{model}}) = \mathbb{E}_{x\sim \hat{p}_{\mathrm{data}}}[\log\hat{p}_{\mathrm{data}}(x) - \log p_{\mathrm{model}}(x)]\\[2ex]
    H(\hat{p}_{\mathrm{data}}, p_{\mathrm{model}}) = -\mathbb{E}_{x\sim \hat{p}_{\mathrm{data}}}[\log p_{\mathrm{model}}(x)]
\end{cases}
$$
Note that $\mathbb{E}_{\hat{p}_{\mathrm{data}}} \to \mathbb{E}_{p_{\mathrm{data}}}$ by [[Law of Large Numbers\|LLN]].

## Conditional Log-Likelihood

The MLE can readily be generalized to the case where our goal is to estimate a conditional probability $P(y|x;θ)$ in order to predict $y$ given $x$. And it forms the basis for most [[Supervised Learning\|Supervised Learning]].

$$
\theta_{ML} = \argmax_\theta P(Y|X;\theta)
$$

Here $X$ represents the inputs and $Y$ represents the observed targets.

If the examples are assumed to be i.i.d., then we have

$$
\theta_{ML} = \argmax_\theta \sum_i\log P(y^i|x^i;\theta)
$$

## Properties of MLE

### Constancy/Invariance

That is, the transformation of a MLE $g(\hat{\theta})$ is still the MSE of $g(\theta)$.

### Consistency

The MLE has the property of [[Estimation#Consistency\|consistency]], if 
- realizability and identifiability: $\exists! \theta^* \in \{\theta\} \text{ s.t. } p_{\mathrm{\mathrm{model}}}(\cdot;\theta) = p_{\mathrm{\mathrm{data}}}$
- or [[Likelihood#Sufficient Regularity Conditions\|Likelihood#Sufficient Regularity Conditions]] are satisfied

### Best Statistical Efficiency

We say a consistent estimator has better ==statistical efficiency==, if it obtains **lower generalization error** for a **fixed number of samples**, or equivalently, requires fewer examples to obtain a fixed level of generalization error.

The **Cramér-Rao lower bound** shows that no consistent estimator has a lower [[Mean Squared Error\|Mean Squared Error]] than the maximum likelihood estimator for a large number of samples.

## Asymptotic Normality

If [[Likelihood#Sufficient Regularity Conditions\|Likelihood#Sufficient Regularity Conditions]] are satisfied, then we have
$$
\frac{\hat{\theta}_{\mathrm{ML}}-\theta}{\sigma_{\mathrm{ML}}} \overset{ d }{ \longrightarrow } \mathcal{N}(0,1),
$$
where the **asymptotic variance** is
$$
\sigma^{2} _{\mathrm{ML}} = I^{-1}_{n}(\theta),
$$
where $I_{n}$ is the [[Fisher Information\|Fisher Information]].

- [@] This property can be used to prove [[Central Limit Theorem\|CLT]] when $\theta = \mu$, $\hat{\theta}_{\mathrm{ML}} = \overline{x}$ and $\sigma$ is known.

## Example - [[Linear Regression\|Linear Regression]]
